import os
import re
import html
import hashlib
import logging
import sys
import asyncio
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse, urlunparse

import feedparser
import requests
import trafilatura
from bs4 import BeautifulSoup
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from rapidfuzz import fuzz
from dotenv import load_dotenv

from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.constants import ChatAction, ParseMode
from telegram.ext import Application, CommandHandler, ContextTypes
from telegram.request import HTTPXRequest

try:
    import yaml
except Exception:
    yaml = None

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ----------
load_dotenv()
TZ = os.environ.get("TZ", "Europe/Amsterdam")
WEEKDAY = int(os.environ.get("WEEKDAY", 0))  # 0 ‚Äî –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ
POST_HOUR = int(os.environ.get("POST_HOUR", 9))
POST_MINUTE = int(os.environ.get("POST_MINUTE", 0))
LOOKBACK_DAYS = int(os.environ.get("LOOKBACK_DAYS", 7))
ITEMS_MIN = int(os.environ.get("ITEMS_MIN", 4))
ITEMS_MAX = int(os.environ.get("ITEMS_MAX", 8))
LANG_PREF = os.environ.get("LANG_PREF", "ru")

BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.environ.get("TELEGRAM_CHANNEL_ID")

# LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
OPENROUTER_MODEL = os.environ.get("OPENROUTER_MODEL", "openai/gpt-4o-mini")
OPENROUTER_SITE_URL = os.environ.get(
    "OPENROUTER_SITE_URL")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è —Ä–µ—Ñ–µ—Ä–∞–ª–∞
OPENROUTER_APP_NAME = os.environ.get(
    "OPENROUTER_APP_NAME")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è —Ä–µ—Ñ–µ—Ä–∞–ª–∞

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# ---------- Telegra.ph API (–±–µ–∑ –≤–Ω–µ—à–Ω–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏) ----------
TELEGRAPH_API = "https://api.telegra.ph"
TELEGRAPH_TOKEN = os.environ.get("TELEGRAPH_ACCESS_TOKEN")
TELEGRAPH_AUTHOR_NAME = os.environ.get(
    "TELEGRAPH_AUTHOR_NAME", "Fashion Digest")
TELEGRAPH_AUTHOR_URL = os.environ.get("TELEGRAPH_AUTHOR_URL")


def tgraph_create_account(short_name: str, author_name: str = "", author_url: str = "") -> dict:
    try:
        r = requests.post(f"{TELEGRAPH_API}/createAccount", data={
            "short_name": short_name[:32] or "digest",
            "author_name": author_name or "",
            "author_url": author_url or "",
        }, timeout=20)
        return r.json()
    except Exception as e:
        logger.warning(f"Telegraph createAccount failed: {e}")
        return {"ok": False, "error": str(e)}


def tgraph_text_to_nodes(text: str) -> list:
    nodes = []
    for para in [p.strip() for p in (text or "").split("\n") if p.strip()]:
        nodes.append({"tag": "p", "children": [para]})
    return nodes


def tgraph_create_page(access_token: str, title: str, nodes: list, author_name: str = "", author_url: str = "") -> dict:
    try:
        import json
        payload = {
            "access_token": access_token,
            "title": (title or "")[:256],
            "author_name": author_name or "",
            "author_url": author_url or "",
            "content": json.dumps(nodes, ensure_ascii=False),  # —Å—Ç—Ä–æ–∫–∞ JSON
            "return_content": False,
        }
        r = requests.post(f"{TELEGRAPH_API}/createPage",
                          data=payload, timeout=30)
        return r.json()
    except Exception as e:
        logger.warning(f"Telegraph createPage failed: {e}")
        return {"ok": False, "error": str(e)}


# ---------- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (RSS) ----------
GOOGLE_NEWS_RU = "https://news.google.com/rss/search?hl=ru&gl=RU&ceid=RU:ru&q="
GOOGLE_NEWS_EN = "https://news.google.com/rss/search?hl=en&gl=US&ceid=US:en&q="

# –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ–±—â—É—é –º–æ–¥—É –∏ –æ–¥–µ–∂–¥—É, —Å–æ—Ö—Ä–∞–Ω—è—è –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –±–µ–ª—å—ë
QUERY_TERMS = [
    # —Ä—É—Å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã ‚Äî –æ–±—â–∞—è –º–æ–¥–∞ –∏ –æ–¥–µ–∂–¥–∞
    "–º–æ–¥–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è OR –ª—É–∫–±—É–∫ OR –ø–æ–∫–∞–∑",
    "streetwear OR —Å—Ç—Ä–∏—Ç–≤–∏—Ä",
    "–∫–∞–ø—Å—É–ª—å–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è OR –∫–∞–ø—Å—É–ª–∞ –º–æ–¥–∞",
    "–±—Ä–µ–Ω–¥ –æ–¥–µ–∂–¥—ã –Ω–æ–≤–∏–Ω–∫–∏ OR —Ä–µ–ª–∏–∑",
    "–ø–ª–∞—Ç—å–µ OR —é–±–∫–∞ OR –±—Ä—é–∫–∏ OR –¥–∂–∏–Ω—Å—ã OR –ø–∏–¥–∂–∞–∫ OR –ø–∞–ª—å—Ç–æ",
    "–æ–±—É–≤—å OR –∫—Ä–æ—Å—Å–æ–≤–∫–∏ OR –±–æ—Ç–∏–Ω–∫–∏ OR —Ç—É—Ñ–ª–∏",
    "–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã —Å—É–º–∫–∞ OR —É–∫—Ä–∞—à–µ–Ω–∏—è OR —Ä–µ–º–µ–Ω—å",
    # –±–µ–ª—å—ë –∏ swim
    "–∂–µ–Ω—Å–∫–æ–µ –±–µ–ª—å–µ OR –±–µ–ª—å—ë",
    "–±—Ä–∞ OR –±—é—Å—Ç–≥–∞–ª—å—Ç–µ—Ä OR –±—Ä–∞–ª–µ—Ç—Ç",
    "—Ç—Ä—É—Å—ã OR —Ç—Ä—É—Å–∏–∫–∏",
    "–∫–æ—Ä—Å–µ—Ç OR –±–æ–¥–∏",
    "–∫—É–ø–∞–ª—å–Ω–∏–∫ fashion",
    "–Ω–∏–∂–Ω–µ–µ –±–µ–ª—å–µ –±—Ä–µ–Ω–¥",
    # –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã ‚Äî –æ–±—â–∞—è –º–æ–¥–∞ –∏ –æ–¥–µ–∂–¥–∞
    "fashion collection OR lookbook OR runway",
    "streetwear drop OR capsule collection",
    "ready-to-wear OR RTW",
    "dress OR skirt OR jeans OR blazer OR coat",
    "footwear sneakers OR boots OR heels",
    "accessories bag OR jewelry OR belt",
    # –±–µ–ª—å—ë –∏ swim (EN)
    "lingerie",
    "panties OR briefs",
    "bra OR bralette",
    "corset OR bodysuit",
    "swimwear fashion",
    "intimates market",
]

STATIC_FEEDS = [
    # –¥–æ–±–∞–≤–ª—è–π—Ç–µ –ø—Ä—è–º—ã–µ RSS –∑–¥–µ—Å—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–º–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å —á–µ—Ä–µ–∑ keywords.yaml)
KEYWORDS = {
    "include": [
        # –æ–±—â–∞—è –º–æ–¥–∞/–æ–¥–µ–∂–¥–∞
        "–º–æ–¥–∞", "fashion", "–ª—É–∫–±—É–∫", "lookbook", "runway", "runway show", "collection", "capsule",
        "–±—Ä–µ–Ω–¥", "—Ä–µ–ª–∏–∑", "–¥—Ä–æ–ø", "drop", "–ø–æ–∫–∞–∑", "–∫–∞–º–ø–∞–Ω–∏—è", "campaign",
        "retail", "–º–∞–≥–∞–∑–∏–Ω", "–≤–∏—Ç—Ä–∏–Ω–∞", "–º–µ—Ä—á–∞–Ω–¥–∞–π–∑–∏–Ω–≥", "seasons", "resort", "pre-fall", "SS25", "FW25",
        "streetwear", "ready-to-wear", "RTW", "couture", "athleisure",
        # –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ–¥–µ–∂–¥—ã
        "–ø–ª–∞—Ç—å–µ", "—é–±–∫–∞", "–±—Ä—é–∫–∏", "–¥–∂–∏–Ω—Å—ã", "–ø–∏–¥–∂–∞–∫", "–∂–∞–∫–µ—Ç", "–ø–∞–ª—å—Ç–æ", "—Ç—Ä–∏–∫–æ—Ç–∞–∂", "—Å–≤–∏—Ç–µ—Ä",
        "–æ–±—É–≤—å", "–∫—Ä–æ—Å—Å–æ–≤–∫–∏", "–±–æ—Ç–∏–Ω–∫–∏", "—Ç—É—Ñ–ª–∏", "—Å–∞–Ω–¥–∞–ª–∏–∏",
        "–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã", "—Å—É–º–∫–∞", "—Ä—é–∫–∑–∞–∫", "—É–∫—Ä–∞—à–µ–Ω–∏—è", "—Ä–µ–º–µ–Ω—å", "–æ—á–∫–∏",
        # –º–∞—Ç–µ—Ä–∏–∞–ª—ã/–∫–∞—á–µ—Å—Ç–≤–∞
        "–º–∞—Ç–µ—Ä–∏–∞–ª—ã", "–∫—Ä—É–∂–µ–≤–æ", "—Å–µ—Ç—á–∞—Ç—ã–π", "sustainability", "eco", "upcycle", "denim", "silk", "wool",
        # –±–µ–ª—å—ë
        "lingerie", "–Ω–∏–∂–Ω–µ–µ –±–µ–ª—å–µ", "bra", "bralette", "–∫–æ—Ä—Å–µ—Ç", "bodysuit", "swimwear",
    ],
    "exclude": [
        "18+", "—ç—Ä–æ—Ç–∏–∫–∞", "–ø–æ—Ä–Ω–æ", "adult", "NSFW", "onlyfans", "–∏–Ω—Ç–∏–º-—É—Å–ª—É–≥–∏"
    ]
}

if yaml is not None:
    try:
        kw_path = os.path.join(os.getcwd(), "keywords.yaml")
        if os.path.isfile(kw_path):
            with open(kw_path, "r", encoding="utf-8") as f:
                loaded = yaml.safe_load(f) or {}
                if isinstance(loaded, dict):
                    KEYWORDS["include"] = list(
                        set(KEYWORDS["include"] + (loaded.get("include") or [])))
                    KEYWORDS["exclude"] = list(
                        set(KEYWORDS["exclude"] + (loaded.get("exclude") or [])))
    except Exception as e:
        logger.warning(f"keywords.yaml load failed: {e}")

# ---------- –£—Ç–∏–ª–∏—Ç—ã ----------


def normalize_url(u: str) -> str:
    try:
        p = urlparse(u)
        query = "&".join([kv for kv in p.query.split(
            "&") if not kv.lower().startswith("utm_") and kv])
        return urlunparse((p.scheme, p.netloc, p.path, "", query, ""))
    except Exception:
        return u


def fetch_rss(url: str):
    try:
        return feedparser.parse(url)
    except Exception as e:
        logger.warning(f"RSS error {url}: {e}")
        return {"entries": []}


def extract_main_text(url: str) -> str:
    try:
        downloaded = trafilatura.fetch_url(url, no_ssl=True)
        if not downloaded:
            return ""
        text = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=False,
            favor_recall=True,
            with_metadata=False,
            output="txt",
        )
        return (text or "").strip()
    except Exception as e:
        logger.warning(f"Extract error {url}: {e}")
        return ""


def relevance_score(title: str, summary: str = "") -> int:
    """0 = –Ω–µ –±—Ä–∞—Ç—å, >=1 = –ø–æ–¥—Ö–æ–¥–∏—Ç; +–±–∞–ª–ª—ã –∑–∞ –±–µ–ª—å—ë."""
    blob = f"{title}\n{summary}".lower()
    if any(bad in blob for bad in [w.lower() for w in KEYWORDS["exclude"]]):
        return 0
    # —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º –æ–±—â—É—é –º–æ–¥—É/–æ–¥–µ–∂–¥—É –ø–æ –∫–ª—é—á–µ–≤–∏–∫–∞–º
    has_fashion = any(w.lower() in blob for w in [
        "fashion", "–º–æ–¥–∞", "runway", "lookbook", "–ª—É–∫–±—É–∫", "–∫–æ–ª–ª–µ–∫—Ü", "–ø–æ–∫–∞–∑", "–±—Ä–µ–Ω–¥", "–∫–∞–º–ø–∞–Ω–∏—è",
        "—Ä–∏—Ç–µ–π–ª", "–º–∞–≥–∞–∑–∏–Ω", "designer", "–¥–∏–∑–∞–π–Ω–µ—Ä", "collab", "–∫–æ–ª–ª–∞–±", "fashion week",
        "streetwear", "ready-to-wear", "rtw", "couture", "athleisure",
        "–ø–ª–∞—Ç—å–µ", "—é–±–∫–∞", "–±—Ä—é–∫–∏", "–¥–∂–∏–Ω—Å—ã", "–ø–∏–¥–∂–∞–∫", "–∂–∞–∫–µ—Ç", "–ø–∞–ª—å—Ç–æ", "—Å–≤–∏—Ç–µ—Ä", "—Ç—Ä–∏–∫–æ—Ç–∞–∂",
        "–æ–±—É–≤—å", "–∫—Ä–æ—Å—Å–æ–≤–∫–∏", "–±–æ—Ç–∏–Ω–∫–∏", "—Ç—É—Ñ–ª–∏", "—Å–∞–Ω–¥–∞–ª–∏–∏", "—Å—É–º–∫–∞", "–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã",
    ]) or any(w.lower() in blob for w in [w.lower() for w in KEYWORDS["include"]])
    if not has_fashion:
        return 0
    score = 1
    # –±–µ–ª—å—ë–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–∞—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–∞–ª–ª, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã
    if any(w in blob for w in [
            "lingerie", "–±–µ–ª—å–µ", "–±–µ–ª—å—ë", "–Ω–∏–∂–Ω–µ–µ –±–µ–ª—å–µ", "bra", "bralette", "–±—é—Å—Ç–≥–∞–ª—å—Ç–µ—Ä",
            "—Ç—Ä—É—Å—ã", "—Ç—Ä—É—Å–∏–∫–∏", "panties", "briefs", "–∫–æ—Ä—Å–µ—Ç", "bodysuit", "swimwear", "—á—É–ª–∫"
    ]):
        score += 1
    return score


def dedupe(items):
    """–£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É URL –∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."""
    seen = set()
    out = []
    for it in items:
        k = normalize_url(it["link"]) or it["link"]
        title = it.get("title", "")
        key = hashlib.md5(k.encode()).hexdigest()
        if key in seen:
            continue
        dup_title = False
        for jt in out:
            if fuzz.partial_ratio(title, jt.get("title", "")) > 90:
                dup_title = True
                break
        if dup_title:
            continue
        seen.add(key)
        out.append(it)
    return out

# ---------- –°–∞–º–º–∞—Ä–∏ ----------


OPENAI_MODEL = "gpt-4o-mini"


def _postprocess_summary(title: str, text: str) -> str:
    # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –ª–∏—à–Ω—é—é –¥–ª–∏–Ω—É
    s = (text or "").strip()
    title_low = (title or "").strip().lower()
    if s.lower().startswith(title_low):
        s = s[len(title):].lstrip(" .:-")
    # —É–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–≤—ã—Ö 60 —Å–∏–º–≤–æ–ª–æ–≤
    if title_low and title_low in s[:120].lower():
        s = re.sub(re.escape(title), "", s, flags=re.IGNORECASE).lstrip(" .:-")
    # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
    if len(s) > 220:
        s = s[:217].rstrip() + "‚Ä¶"
    return s


def summarize(title: str, text: str, lang_pref: str = "ru") -> str:
    # 1) OpenRouter
    if OPENROUTER_API_KEY and len(text) > 200:
        try:
            import json
            import urllib.request

            sys_prompt = (
                "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –º–æ–¥–Ω–æ–≥–æ –º–µ–¥–∏–∞. –ö—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏ –Ω–æ–≤–æ—Å—Ç—å –ø–æ –º–æ–¥–µ/–æ–¥–µ–∂–¥–µ/–∞–∫—Å–µ—Å—Å—É–∞—Ä–∞–º. "
                "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞–≥–æ–ª–æ–≤–æ–∫. 1 –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ + 2 –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—É–Ω–∫—Ç–∞. "
                "–¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã (–±—Ä–µ–Ω–¥/–∫–æ–ª–ª–µ–∫—Ü–∏—è/–¥–∞—Ç—ã/—Ü–µ–Ω—ã, –µ—Å–ª–∏ –µ—Å—Ç—å). –Ø–∑—ã–∫: " +
                ("—Ä—É—Å—Å–∫–∏–π" if lang_pref != "en" else "English") + "."
            )
            user_prompt = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {text[:6000]}"
            body = json.dumps({
                "model": OPENROUTER_MODEL,
                "messages": [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,
            }).encode("utf-8")
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            }
            if OPENROUTER_SITE_URL:
                headers["HTTP-Referer"] = OPENROUTER_SITE_URL
            if OPENROUTER_APP_NAME:
                headers["X-Title"] = OPENROUTER_APP_NAME
            req = urllib.request.Request(
                url="https://openrouter.ai/api/v1/chat/completions",
                data=body,
                headers=headers,
                method="POST",
            )
            with urllib.request.urlopen(req, timeout=40) as resp:
                import json as _json
                j = _json.loads(resp.read())
                return _postprocess_summary(title, j["choices"][0]["message"]["content"].strip())
        except Exception as e:
            logger.warning(f"OpenRouter summary failed: {e}")
    # 2) OpenAI (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á)
    if OPENAI_API_KEY and len(text) > 200:
        try:
            import json
            import urllib.request

            sys_prompt = (
                "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –º–æ–¥–Ω–æ–≥–æ –º–µ–¥–∏–∞. –ö—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏ –Ω–æ–≤–æ—Å—Ç—å –ø–æ –º–æ–¥–µ/–æ–¥–µ–∂–¥–µ/–∞–∫—Å–µ—Å—Å—É–∞—Ä–∞–º. "
                "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞–≥–æ–ª–æ–≤–æ–∫. 1 –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ + 2 –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—É–Ω–∫—Ç–∞. "
                "–¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã (–±—Ä–µ–Ω–¥/–∫–æ–ª–ª–µ–∫—Ü–∏—è/–¥–∞—Ç—ã/—Ü–µ–Ω—ã, –µ—Å–ª–∏ –µ—Å—Ç—å). –Ø–∑—ã–∫: " +
                ("—Ä—É—Å—Å–∫–∏–π" if lang_pref != "en" else "English") + "."
            )
            user_prompt = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {text[:6000]}"
            body = json.dumps({
                "model": OPENAI_MODEL,
                "messages": [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,
            }).encode("utf-8")
            req = urllib.request.Request(
                url="https://api.openai.com/v1/chat/completions",
                data=body,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                },
                method="POST",
            )
            with urllib.request.urlopen(req, timeout=40) as resp:
                import json as _json
                j = _json.loads(resp.read())
                return _postprocess_summary(title, j["choices"][0]["message"]["content"].strip())
        except Exception as e:
            logger.warning(f"OpenAI summary failed: {e}")
    # 3) Fallback: —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ –∫–æ—Ä–æ—Ç–∫–æ
    sentences = re.split(r"(?<=[.!?])\s+", text)
    core_sentences = [s for s in sentences if len(s.split()) > 6][:2]
    core = " ".join(core_sentences).strip()
    return _postprocess_summary(title, core or "")

# ---------- –°–±–æ—Ä –¥–∞–π–¥–∂–µ—Å—Ç–∞ ----------


def collect_sources():
    feeds = list(STATIC_FEEDS)
    for q in QUERY_TERMS:
        if LANG_PREF in ("ru", "both"):
            feeds.append(GOOGLE_NEWS_RU + requests.utils.quote(q))
        if LANG_PREF in ("en", "both"):
            feeds.append(GOOGLE_NEWS_EN + requests.utils.quote(q))
    return feeds


async def gather_candidates():
    items = []
    since = datetime.utcnow() - timedelta(days=LOOKBACK_DAYS)
    feeds = collect_sources()
    for f in feeds:
        d = fetch_rss(f)
        for e in d.get("entries", []):
            link = e.get("link")
            title = html.unescape(e.get("title", "").strip())
            summary_html = e.get("summary", "") or e.get("description", "")
            summary = BeautifulSoup(
                summary_html, "html.parser").get_text(" ")[:300]
            if not link or not title:
                continue
            published = None
            if e.get("published_parsed"):
                published = datetime(*e.published_parsed[:6])
            elif e.get("updated_parsed"):
                published = datetime(*e.updated_parsed[:6])
            else:
                published = datetime.utcnow()
            if published < since:
                continue
            if relevance_score(title, summary) == 0:
                continue
            items.append({
                "title": title,
                "summary": summary,
                "link": link,
                "published": published,
                "source": "rss",
            })
    items.sort(key=lambda x: x["published"], reverse=True)
    return dedupe(items)


async def build_digest(max_items=None):
    # –∂—ë—Å—Ç–∫–∏–π –ª–∏–º–∏—Ç 5
    hard_cap = 5
    max_items = min(ITEMS_MAX or hard_cap, hard_cap) if max_items is None else min(
        max_items, hard_cap)

    cand = await gather_candidates()
    out = []
    for it in cand:
        if len(out) >= max_items:
            break
        full_text = extract_main_text(it["link"]) or it.get("summary", "")
        # Telegra.ph: –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ç–æ–∫–µ–Ω–∞ –ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –∞–∫–∫–∞—É–Ω—Ç
        t_url = None
        if full_text and len(full_text) > 200:
            token = os.environ.get("TELEGRAPH_ACCESS_TOKEN")
            if not token:
                acc = tgraph_create_account(
                    short_name=(TELEGRAPH_AUTHOR_NAME or "digest")[:32],
                    author_name=TELEGRAPH_AUTHOR_NAME or "",
                    author_url=TELEGRAPH_AUTHOR_URL or "",
                )
                if acc.get("ok") and acc.get("result"):
                    token = acc["result"].get("access_token")
                    if token:
                        os.environ["TELEGRAPH_ACCESS_TOKEN"] = token
            if token:
                nodes = tgraph_text_to_nodes(full_text)
                resp = tgraph_create_page(
                    access_token=token,
                    title=it["title"],
                    nodes=nodes,
                    author_name=TELEGRAPH_AUTHOR_NAME,
                    author_url=TELEGRAPH_AUTHOR_URL,
                )
                if resp.get("ok") and resp.get("result"):
                    t_url = resp["result"].get("url")
        s = summarize(it["title"], full_text, LANG_PREF)
        out.append({**it, "summary2": s, "tgraph": t_url})
    return out


def pick_emoji(title: str, text: str) -> str:
    blob = f"{title}\n{text}".lower()
    if any(k in blob for k in ["runway", "–ø–æ–∫–∞–∑", "fashion week", "–ø–æ–¥–∏—É–º", "–Ω–µ–¥–µ–ª—è –º–æ–¥—ã"]):
        return "üëó"
    if any(k in blob for k in ["–∫–æ–ª–ª–µ–∫—Ü", "collection", "–∫–∞–ø—Å—É–ª", "–∫–∞–ø—Å—É–ª–∞", "lookbook", "–ª—É–∫–±—É–∫", "drop", "–¥—Ä–æ–ø"]):
        return "üßµ"
    if any(k in blob for k in ["—Ä–µ—Ç–µ–π–ª", "–º–∞–≥–∞–∑–∏–Ω", "retail", "–º–∞—Ä–∫–µ—Ç", "–ø—Ä–æ–¥–∞–∂"]):
        return "üõçÔ∏è"
    if any(k in blob for k in ["–≤–∏–¥–µ–æ", "video", "–∫–∞–º–ø–∞–Ω–∏—è", "campaign", "–∫—Ä–µ–∞—Ç–∏–≤", "—Å—ä—ë–º–∫", "—Å—ä–µ–º–∫"]):
        return "üé¨"
    if any(k in blob for k in ["–¥–∂–∏–Ω—Å", "–±—Ä—é–∫–∏", "–ø–∞–ª—å—Ç–æ", "–ø–∏–¥–∂–∞–∫", "—Å—É–º–∫–∞", "–∞–∫—Å–µ—Å—Å—É–∞—Ä", "–æ–±—É–≤—å", "–∫—Ä–æ—Å—Å–æ–≤", "–±–æ—Ç–∏–Ω–∫", "heels"]):
        return "üß•"
    if any(k in blob for k in ["–±–µ–ª—å–µ", "–±–µ–ª—å—ë", "lingerie", "bra", "bralette", "–∫–æ—Ä—Å–µ—Ç", "bodysuit", "swimwear"]):
        return "ü©±"
    return "‚ú®"


def create_digest_page(title: str, items: list) -> str | None:
    try:
        # –ó–∞–≥–æ—Ç–æ–≤–∫–∞ –±–æ–ª—å—à–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ c –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ–º –∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏
        nodes = []
        # H2 –∑–∞–≥–æ–ª–æ–≤–æ–∫
        nodes.append({"tag": "h2", "children": [title]})
        # –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫‚Äë–∞–Ω–æ–Ω—Å
        lead = (
            "–ù–µ–¥–µ–ª—è ‚Äî –ø—Ä–æ —Å—Ç–∏–ª—å, —Å–∏–ª—É—ç—Ç—ã –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã. –ù–∏–∂–µ ‚Äî –∫—Ä–∞—Ç–∫–∏–µ –≤—ã–∂–∏–º–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø–æ–≥—Ä—É–∂–µ–Ω–∏—è."
        )
        nodes.append({"tag": "p", "children": [lead]})
        nodes.append({"tag": "p", "children": [""]})
        # –ù–∞–≤–∏–≥–∞—Ü–∏—è
        nodes.append({"tag": "h3", "children": ["üß≠ –ù–∞–≤–∏–≥–∞—Ü–∏—è"]})
        toc_list = []
        for idx, it in enumerate(items, 1):
            sec_id = f"sec-{idx}"
            toc_list.append({
                "tag": "li",
                "children": [
                    {"tag": "a", "attrs": {"href": f"#{sec_id}"},
                        "children": [it["title"]]}
                ],
            })
        nodes.append({"tag": "ul", "children": toc_list})
        nodes.append({"tag": "p", "children": [""]})
        # –†–∞–∑–¥–µ–ª—ã
        for idx, it in enumerate(items, 1):
            sec_id = f"sec-{idx}"
            emoji = pick_emoji(it.get("title", ""), it.get("summary2", ""))
            # –Ø–∫–æ—Ä—å (Telegraph –º–æ–∂–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å id; –æ—Å—Ç–∞–≤–∏–º —Å—Å—ã–ª–∫—É-—è–∫–æ—Ä—å –ø–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º)
            nodes.append({"tag": "p", "children": [
                {"tag": "a", "attrs": {"name": sec_id}, "children": [""]}
            ]})
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞ –∫–∞–∫ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª (—É–º–Ω–∞—è —Å—Å—ã–ª–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ)
            nodes.append({
                "tag": "h3",
                "children": [
                    f"{emoji} ",
                    {"tag": "a", "attrs": {
                        "href": it["link"]}, "children": [it["title"]]},
                ],
            })
            # –ö–æ—Ä–æ—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞
            if it.get("summary2"):
                nodes.append({"tag": "p", "children": [it["summary2"]]})
            # –î–∞—Ç–∞ (–º–µ–ª–∫–∏–º) –∏ —Å–∫—Ä—ã—Ç–∞—è —É–º–Ω–∞—è —Å—Å—ã–ª–∫–∞
            date_str = it["published"].strftime(
                "%d %b %Y") if it.get("published") else ""
            nodes.append({
                "tag": "p",
                "children": [
                    {"tag": "em", "children": [f"{date_str}"]}, " ‚Äî ",
                    {"tag": "a", "attrs": {"href": it["link"]}, "children": [
                        "–ø–µ—Ä–µ–π—Ç–∏ –∫ –º–∞—Ç–µ—Ä–∏–∞–ª—É"]},
                ],
            })
            nodes.append({"tag": "p", "children": [""]})
        # –ò—Ç–æ–≥–∏
        nodes.append({"tag": "h3", "children": ["–ò—Ç–æ–≥–∏ –Ω–µ–¥–µ–ª–∏"]})
        nodes.append({
            "tag": "p",
            "children": [
                "–ì–ª–∞–≤–Ω–æ–µ: –≤—ã–±–∏—Ä–∞–µ–º –Ω–æ—Å–∏–±–µ–ª—å–Ω—ã–µ —Å–∏–ª—É—ç—Ç—ã, —Ñ–∏–∫—Å–∏—Ä—É–µ–º –∫–∞–ø—Å—É–ª—ã –∏ –≥–æ—Ç–æ–≤–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∑–∞–ø–∞—Å–æ–º –æ–±—Ä–∞–∑–æ–≤. "
                "–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∏–º–µ—Ä–∫–∞–º –∏ —Ç–µ—Å—Ç–∞–º: —á—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–π–¥—ë—Ç –∞—É–¥–∏—Ç–æ—Ä–∏–∏ ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –≤ –ø–ª–∞–Ω–∞—Ö."
            ],
        })

        token = os.environ.get("TELEGRAPH_ACCESS_TOKEN")
        if not token:
            acc = tgraph_create_account(
                short_name=(TELEGRAPH_AUTHOR_NAME or "digest")[:32],
                author_name=TELEGRAPH_AUTHOR_NAME or "",
                author_url=TELEGRAPH_AUTHOR_URL or "",
            )
            if acc.get("ok") and acc.get("result"):
                token = acc["result"].get("access_token")
                if token:
                    os.environ["TELEGRAPH_ACCESS_TOKEN"] = token
        if not token:
            return None
        resp = tgraph_create_page(
            access_token=token,
            title=title,
            nodes=nodes,
            author_name=TELEGRAPH_AUTHOR_NAME,
            author_url=TELEGRAPH_AUTHOR_URL,
        )
        if resp.get("ok") and resp.get("result"):
            return resp["result"].get("url")
    except Exception as e:
        logger.warning(f"Digest page failed: {e}")
    return None


# ---------- Telegram ----------


async def send_typing(bot, chat_id):
    try:
        await bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
    except Exception:
        pass


def render_message(items, digest_url: str | None = None):
    if not items:
        return "üôà –ó–∞ –Ω–µ–¥–µ–ª—é –Ω–∏—á–µ–≥–æ –¥–æ—Å—Ç–æ–π–Ω–æ–≥–æ. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏?"
    # –ë–µ–∑ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–æ–π —Ñ—Ä–∞–∑—ã ‚Äî —Å—Ä–∞–∑—É –ø—É–Ω–∫—Ç—ã
    bullets = []
    for it in items:
        emoji = pick_emoji(it.get("title", ""), it.get("summary2", ""))
        line = f"{emoji} {it['title']}: {it['summary2']}"
        bullets.append(line)
    cta = "\n–î–µ–π—Å—Ç–≤–∏–µ: –æ—Ç–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–∑—ã –∏ –ø–ª–∞–Ω–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –Ω–µ–¥–µ–ª—é.\n"
    link = f"\n–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ä–∞–∑–±–æ—Ä: {digest_url}" if digest_url else ""
    return "\n\n".join(bullets) + "\n\n" + cta + link


def build_keyboard(items, digest_url: str | None):
    # –û–¥–Ω–∞ –∫–Ω–æ–ø–∫–∞ –Ω–∞ –æ–±—â–∏–π —Ä–∞–∑–±–æ—Ä; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –∫–Ω–æ–ø–∫–∏ –ø–æ –ø—É–Ω–∫—Ç–∞–º
    if digest_url:
        return InlineKeyboardMarkup([[InlineKeyboardButton(text="‚ö° –ü–û–°–ú–û–¢–†–ï–¢–¨", url=digest_url)]])
    buttons = []
    for i, it in enumerate(items, 1):
        if it.get("tgraph"):
            buttons.append([InlineKeyboardButton(
                text=f"–ß–∏—Ç–∞—Ç—å {i}", url=it["tgraph"])])
    return InlineKeyboardMarkup(buttons) if buttons else None


async def post_digest(bot):
    await send_typing(bot, CHANNEL_ID)
    try:
        items = await build_digest()
        digest_url = create_digest_page("–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç", items)
        text = render_message(items, digest_url)
        keyboard = build_keyboard(items, digest_url)
        await bot.send_message(
            chat_id=CHANNEL_ID,
            text=text,
            parse_mode=ParseMode.MARKDOWN,
            disable_web_page_preview=not digest_url,
            reply_markup=keyboard,
        )
        logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {len(items)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤")
    except Exception as e:
        logger.exception(e)
        await bot.send_message(chat_id=CHANNEL_ID, text=f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")


# ---------- –ö–æ–º–∞–Ω–¥—ã ----------

async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø —Å–æ–±–∏—Ä–∞—é –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ –º–æ–¥–µ –∏ –æ–¥–µ–∂–¥–µ (–≤–∫–ª—é—á–∞—è –±–µ–ª—å—ë). –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /preview –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∞."
    )


async def cmd_preview(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await send_typing(context.bot, update.effective_chat.id)
    try:
        items = await build_digest()
        text = render_message(items)
        await update.message.reply_text(text, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=False)
    except Exception as e:
        logger.exception(e)
        await update.message.reply_text(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–≤—å—é: {e}")


# ---------- –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ ----------

async def on_start(app: Application):
    scheduler = AsyncIOScheduler(timezone=TZ)
    trigger = CronTrigger(day_of_week=WEEKDAY,
                          hour=POST_HOUR, minute=POST_MINUTE)
    scheduler.add_job(post_digest, trigger, args=[app.bot], id="weekly_digest")
    scheduler.start()
    logger.info(
        f"–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω: WEEKDAY={WEEKDAY} {POST_HOUR}:{POST_MINUTE} ({TZ})")


async def preview_console():
    items = await build_digest()
    digest_url = create_digest_page("–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç", items)
    text = render_message(items, digest_url)
    print(text)


async def post_once():
    request = HTTPXRequest(
        connect_timeout=30.0,
        read_timeout=30.0,
        write_timeout=30.0,
        pool_timeout=30.0,
    )
    app = Application.builder().token(BOT_TOKEN).request(request).build()
    await app.initialize()
    try:
        await post_digest(app.bot)
    finally:
        try:
            await app.shutdown()
        except Exception:
            pass


def main():
    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –ø—Ä–µ–≤—å—é-—Ä–µ–∂–∏–º –±–µ–∑ Telegram –∏ –±–µ–∑ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
    if os.environ.get("PREVIEW_CONSOLE") == "1" or (len(sys.argv) > 1 and sys.argv[1].lower() == "preview"):
        asyncio.run(preview_console())
        return

    # –û–¥–Ω–æ—Ä–∞–∑–æ–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è –≤ –∫–∞–Ω–∞–ª –±–µ–∑ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
    if os.environ.get("POST_ONCE") == "1" or (len(sys.argv) > 1 and sys.argv[1].lower() == "post"):
        if not BOT_TOKEN or not CHANNEL_ID:
            raise RuntimeError(
                "–î–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω—É–∂–Ω—ã TELEGRAM_BOT_TOKEN –∏ TELEGRAM_CHANNEL_ID")
        asyncio.run(post_once())
        return

    if not BOT_TOKEN:
        raise RuntimeError("TELEGRAM_BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
    if not CHANNEL_ID:
        raise RuntimeError("TELEGRAM_CHANNEL_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
    request = HTTPXRequest(
        connect_timeout=30.0,
        read_timeout=30.0,
        write_timeout=30.0,
        pool_timeout=30.0,
    )
    app = Application.builder().token(BOT_TOKEN).request(request).build()
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("preview", cmd_preview))
    app.post_init = on_start
    logger.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è‚Ä¶")
    app.run_polling(close_loop=False)


if __name__ == "__main__":
    main()
