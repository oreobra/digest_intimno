# main.py ‚Äî TG-–∫–∞–Ω–∞–ª—ã (Telethon) + OpenRouter gpt-4o-mini
# –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –¢–ì –∏ –¥–ª–∏–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –≤ Telegraph (inline-—Å—Å—ã–ª–∫–∏)
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø: —Å—Å—ã–ª–∫–∏ —Ç–µ–ø–µ—Ä—å –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫ –≤ Telegram, –≤ Telegram-–ø–æ—Å—Ç–µ –ù–ï–¢ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π/–ª–∏–¥–æ–≤

import os
import re
import html
import hashlib
import logging
import sys
import asyncio
import random
import json
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse

import requests
# –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —É–±—Ä–∞–Ω: –∑–∞–ø—É—Å–∫ –ø–æ CRON —Å–Ω–∞—Ä—É–∂–∏ (Beget)
from rapidfuzz import fuzz
from dotenv import load_dotenv

from telegram import Update
from telegram.constants import ChatAction, ParseMode
from telegram.ext import Application, CommandHandler, ContextTypes
from telegram.request import HTTPXRequest

# --- Telethon (—á—Ç–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –∫–∞–Ω–∞–ª–æ–≤)
from telethon import TelegramClient
from telethon.sessions import StringSession
from telethon.errors import ChannelPrivateError, ChatAdminRequiredError

load_dotenv()

# ---------- –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ ----------
TZ = os.environ.get("TZ", "Europe/Amsterdam")
WEEKDAY = int(os.environ.get("WEEKDAY", 0))       # 0 ‚Äî –≤—Å
POST_HOUR = int(os.environ.get("POST_HOUR", 9))
POST_MINUTE = int(os.environ.get("POST_MINUTE", 0))
LOOKBACK_DAYS = int(os.environ.get("LOOKBACK_DAYS", 7))
ITEMS_MAX = int(os.environ.get("ITEMS_MAX", 6))
LANG_PREF = os.environ.get("LANG_PREF", "ru")

BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.environ.get("TELEGRAM_CHANNEL_ID")

# OpenRouter
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
OPENROUTER_MODEL = os.environ.get("OPENROUTER_MODEL", "openai/gpt-4o-mini")
OPENROUTER_SITE_URL = os.environ.get("OPENROUTER_SITE_URL")
OPENROUTER_APP_NAME = os.environ.get("OPENROUTER_APP_NAME")

# Telethon —É—á—ë—Ç–∫–∞
TG_API_ID = int(os.environ.get("TELEGRAM_API_ID", "0"))
TG_API_HASH = os.environ.get("TELEGRAM_API_HASH")
TG_STRING_SESSION = os.environ.get("TELEGRAM_STRING_SESSION")

# Telegraph
TELEGRAPH_API = "https://api.telegra.ph"
TELEGRAPH_AUTHOR_NAME = os.environ.get(
    "TELEGRAPH_AUTHOR_NAME", "Fashion Digest")
TELEGRAPH_AUTHOR_URL = os.environ.get("TELEGRAPH_AUTHOR_URL")
TELEGRAPH_ACCESS_TOKEN = os.environ.get("TELEGRAPH_ACCESS_TOKEN")

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# ---------- –ö–∞–Ω–∞–ª—ã Telegram ----------
TG_CHANNELS = [
    "wearfeelings",
    "devilwearshandm",
    "theblueprintnews",
    "goldchihuahua",
    "fashion_mur",
]

# ---------- –£—Ç–∏–ª–∏—Ç—ã —Ç–µ–∫—Å—Ç–∞ ----------
SITE_TAIL_RE = re.compile(r"\s*[-‚Äì‚Äî‚Ä¢]\s*[^\n]{2,50}$")
WS_RE = re.compile(r"\s+")


def clean_title(title: str) -> str:
    t = (title or "").strip()
    t = html.unescape(t)
    t = SITE_TAIL_RE.sub("", t).strip(" .‚Äì‚Äî-¬∑‚Ä¢")
    parts = re.split(r"\s*[:‚Äì‚Äî-]\s*", t)
    if len(parts) == 2 and fuzz.ratio(parts[0], parts[1]) > 92:
        t = parts[0]
    lines = [x.strip() for x in re.split(r"\s{2,}|\n+", t) if x.strip()]
    if len(lines) >= 2 and fuzz.ratio(lines[0], lines[1]) > 92:
        t = lines[0]
    return WS_RE.sub(" ", t).strip()


def title_key_for_dedupe(t: str) -> str:
    t = clean_title(t).lower()
    t = re.sub(r"[^\w\s]", " ", t)
    t = WS_RE.sub(" ", t).strip()
    return t


def dedupe(items):
    seen = set()
    out = []
    for it in items:
        tclean = clean_title(it.get("title", ""))
        if not tclean:
            continue
        host = urlparse(it.get("link", "")).netloc.lower(
        ) or it.get("channel", "tg")
        k = hashlib.md5(
            f"{title_key_for_dedupe(tclean)}|{host}".encode()).hexdigest()
        k2 = hashlib.md5(title_key_for_dedupe(tclean).encode()).hexdigest()
        is_dup = any(fuzz.partial_ratio(tclean, jt.get(
            "title_clean", "")) > 92 for jt in out)
        if is_dup or k in seen or k2 in seen:
            continue
        it["title"] = tclean
        it["title_clean"] = tclean
        seen.add(k)
        seen.add(k2)
        out.append(it)
    return out


# ---------- –§–∏–ª—å—Ç—Ä—ã ----------
BAD_NEWS_RE = re.compile(
    r"\b(–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä|—Å—É–¥|—à—Ç—Ä–∞—Ñ|—É–≥–æ–ª–æ–≤|—Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω|–æ–±–≤–∏–Ω|–ø–æ–¥–æ–∑—Ä–µ–≤|—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü|–Ω–∞—Ä—É—à–µ–Ω[–∏—å—è])\w*\b",
    flags=re.IGNORECASE
)
FASHION_PASS_RE = re.compile(
    r"\b(–∫–æ–ª–ª–µ–∫—Ü|–∫–∞–º–ø–∞–Ω|–ª—É–∫–±—É–∫|–ø–æ–∫–∞–∑|runway|lookbook|editorial|–∫–∞–ø—Å—É–ª|capsule|–ª–∏–Ω[–≥–≥]–µ—Ä[–∏–µ]|–±–µ–ª—å[–µ—ë]|–±—Ä–µ–Ω–¥|–∫–æ–ª–ª–∞–±|drop|–¥—Ä–æ–ø)\w*\b",
    flags=re.IGNORECASE
)
AD_RE = re.compile(
    r"\b(—Ä–µ–∫–ª–∞–º–∞|—Å–ø–æ–Ω—Å–æ—Ä—Å—Ç–≤|–ø–∞—Ä—Ç–Ω[–µ—ë]—Ä—Å–∫|promocod|–ø—Ä–æ–º–æ–∫–æ–¥|erid|–∏–Ω–Ω\b|—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç|—Ä–æ–∑—ã–≥—Ä—ã—à|—Ä–∞–∑—ã–≥—Ä—ã–≤–∞|—Å–∫–∏–¥–∫|–∫—É–ø–æ–Ω|–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å|–ø–æ–¥–ø–∏—Å–∫–∞)\b",
    flags=re.IGNORECASE
)
FASHION_CORE_RE = re.compile(
    r"\b(–º–æ–¥[–∞–∏]|fashion|couture|–¥–∏–∑–∞–π–Ω|–¥–∏–∑–∞–π–Ω–µ—Ä|–ª—É–∫[ -]?–±—É–∫|lookbook|–∫–∞–º–ø–∞–Ω|–∫–æ–ª–ª–µ–∫—Ü|runway|–ø–æ–∫–∞–∑|editorial|–∫–∞–ø—Å—É–ª|capsule|–∫—Ä–æ[–π—è]|—Å–∏–ª—É—ç—Ç|—Ç—Ä–∏–∫–æ—Ç–∞–∂|–¥–µ–Ω–∏–º|–¥–∂–∏–Ω—Å|–ø–∞–ª—å—Ç–æ|–ø–∏–¥–∂–∞–∫|—Å—É–º–∫|–∞–∫—Å–µ—Å—Å—É–∞—Ä|–æ–±—É–≤|–±–æ—Ç–∏–Ω–∫|–∫—Ä–æ—Å—Å–æ–≤|–ø–ª–∞—Ç—å|—é–±–∫|–±—Ä—é–∫|–∫–∞—Ä–¥–∏–≥–∞–Ω|—Ç—Ä–µ–Ω—á|–ø—É—Ö–æ–≤–∏–∫|–∫—Ä–µ–∞—Ç–∏–≤–Ω\w+ –¥–∏—Ä–µ–∫—Ç–æ—Ä|–±—Ä–µ–Ω–¥)\b",
    flags=re.IGNORECASE
)


def is_soft_relevant(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return False
    if AD_RE.search(t):
        return False
    if BAD_NEWS_RE.search(t) and not (FASHION_PASS_RE.search(t) or FASHION_CORE_RE.search(t)):
        return False
    if not (FASHION_PASS_RE.search(t) or FASHION_CORE_RE.search(t)):
        return False
    if len(re.sub(r"\s+", "", t)) < 25:
        return False
    return True


# ---------- –¢–æ–Ω / –∑–∞—á–∏—Å—Ç–∫–∞ ----------
POLICY_TRIGGERS = [
    r"\b—ç—Ä–æ—Ç\w*", r"\b–ø–æ—Ä–Ω–æ\w*", r"\b–≥–æ–ª(—ã–π|–∞—è|—ã–µ)\b", r"\b–æ–≥–æ–ª–µ–Ω\w*",
    r"\b–∏–Ω—Ç–∏–º\w*", r"\bNSFW\b", r"\b–Ω—é\b", r"\b–æ–±–Ω–∞–∂\w*",
]
TRIGGER_RE = re.compile("|".join(POLICY_TRIGGERS), flags=re.IGNORECASE)


def scrub_for_policy(text: str) -> str:
    t = (text or "")
    t = re.sub(r"https?://\S+|t\.me/\S+|www\.\S+", "", t)
    t = re.sub(r"@[A-Za-z0-9_]+", "", t)
    t = TRIGGER_RE.sub(lambda m: m.group(0)[0] + "‚Ä¶", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def fashion_tone_prompt(lang_pref: str = "ru") -> str:
    base = (
        "–ü–∏—à–∏ –∫–∞–∫ –∂—É—Ä–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä: —É–≤–µ—Ä–µ–Ω–Ω–æ, –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –ø–æ –¥–µ–ª—É. "
        "–ò–Ω—Å–∞–π–¥–µ—Ä—Å–∫–∏–π —Ç–æ–Ω, –ª—ë–≥–∫–∞—è –∏—Ä–æ–Ω–∏—è –±–µ–∑ —Å–∞—Ä–∫–∞–∑–º–∞. "
        "–ö–æ—Ä–æ—Ç–∫–∏–µ –∂–∏–≤—ã–µ —Ñ—Ä–∞–∑—ã, –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –º–∞—Ç–µ—Ä–∏–∞–ª—ã, —Å–∏–ª—É—ç—Ç—ã, –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—Ä–µ–Ω–¥–∞. "
        "–ë–µ–∑ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π, —ç–º–æ–¥–∑–∏ –∏ –∫–ª–∏—à–µ. –ù–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è, –∞–∫—Ç–∏–≤–Ω—ã–π –∑–∞–ª–æ–≥."
    )
    if lang_pref == "en":
        base = (
            "Magazine-editor tone: assured, concise, insider. "
            "Light wit, no sarcasm. Concrete details: materials, silhouettes, brand context. "
            "No exclamation marks or emojis. Present tense, active voice."
        )
    return base


def safer_tone_prompt(lang_pref: str = "ru") -> str:
    return (
        fashion_tone_prompt(lang_pref)
        + " –ò–∑–±–µ–≥–∞–π —Å–µ–∫—Å—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ–±–Ω–∞–∂—ë–Ω–Ω–æ–≥–æ —Ç–µ–ª–∞; "
        + "–≥–æ–≤–æ—Ä–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ –æ –º–æ–¥–µ, —Å–∏–ª—É—ç—Ç–∞—Ö, –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±—Ä–µ–Ω–¥–∞."
    )


# ---------- –†–æ—Ç–∞—Ü–∏—è –≥–ª–∞–≥–æ–ª–æ–≤ ----------
ALT_VERBS = [
    "–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç", "–≤—ã–≤–æ–¥–∏—Ç", "–æ—Ç–∫—Ä—ã–≤–∞–µ—Ç", "–¥–µ—Ä–∂–∏—Ç —Ñ–æ–∫—É—Å",
    "—Å–æ–±–∏—Ä–∞–µ—Ç", "–¥–µ–ª–∞–µ—Ç —Å—Ç–∞–≤–∫—É", "–∏–≥—Ä–∞–µ—Ç", "–ø–æ–¥—Å–≤–µ—á–∏–≤–∞–µ—Ç",
    "—Å–¥–≤–∏–≥–∞–µ—Ç –∞–∫—Ü–µ–Ω—Ç", "–ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç", "—É–∫—Ä—É–ø–Ω—è–µ—Ç —Å–∏–ª—É—ç—Ç", "—Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ñ–∞–∫—Ç—É—Ä–æ–π"
]


def vary_verbs(s: str) -> str:
    if not s:
        return s
    if re.search(r"\b(–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç|–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç)\b", s, flags=re.IGNORECASE) and random.random() < 0.5:
        alt = random.choice(ALT_VERBS)
        return re.sub(r"\b(–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç|–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç)\b", alt, s, count=1, flags=re.IGNORECASE)
    return s


# ---------- –í—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ----------
def _clip(s: str, n: int) -> str:
    s = (s or "").strip()
    return (s[: n-1].rstrip() + "‚Ä¶") if len(s) > n else s


def _postprocess_line(title: str, line: str, max_len: int) -> str:
    s = (line or "").strip()
    if title and title.lower() in s.lower():
        s = re.sub(re.escape(title), "", s,
                   flags=re.IGNORECASE).strip(" .:‚Äì‚Äî-")
    s = WS_RE.sub(" ", s).strip()
    return _clip(s, max_len)


# ---------- OpenRouter ----------
def llm_call(messages: list[dict], temperature: float = 0.3, timeout: int = 45) -> str:
    if not OPENROUTER_API_KEY:
        return ""
    body = {"model": OPENROUTER_MODEL,
            "messages": messages, "temperature": temperature}
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    }
    if OPENROUTER_SITE_URL:
        headers["HTTP-Referer"] = OPENROUTER_SITE_URL
    if OPENROUTER_APP_NAME:
        headers["X-Title"] = OPENROUTER_APP_NAME

    try:
        r = requests.post("https://openrouter.ai/api/v1/chat/completions",
                          headers=headers, json=body, timeout=timeout)
        j = r.json()
        if r.status_code == 200 and isinstance(j, dict) and j.get("choices"):
            return j["choices"][0]["message"]["content"] or ""
        if r.status_code == 400 and "filtered" in json.dumps(j).lower():
            safe_msgs = []
            for m in messages:
                if m["role"] == "system":
                    safe_msgs.append(
                        {"role": "system", "content": safer_tone_prompt(LANG_PREF)})
                elif m["role"] == "user":
                    content = m["content"]
                    if isinstance(content, str):
                        content = scrub_for_policy(content)[:2000]
                    safe_msgs.append({"role": "user", "content": content})
                else:
                    safe_msgs.append(m)
            r2 = requests.post("https://openrouter.ai/api/v1/chat/completions",
                               headers=headers,
                               json={"model": OPENROUTER_MODEL,
                                     "messages": safe_msgs,
                                     "temperature": max(0.1, temperature - 0.1)},
                               timeout=timeout)
            j2 = r2.json()
            if r2.status_code == 200 and isinstance(j2, dict) and j2.get("choices"):
                return j2["choices"][0]["message"]["content"] or ""
        logger.warning(f"OpenRouter HTTP {r.status_code}: {j}")
    except Exception as e:
        logger.warning(f"OpenRouter call failed: {e}")
    return ""


# ---------- –í—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–±–æ–ª—å—à–µ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ Telegram) ----------
def generate_intro_line(items: list, lang_pref: str = "ru") -> str:
    topics = []
    for it in items[:8]:
        t = it.get("title") or ""
        if t:
            topics.append(t.strip())
    topics_blob = "\n".join(f"‚Ä¢ {t}" for t in topics) if topics else ""
    raw = llm_call([
        {"role": "system", "content": safer_tone_prompt(
            lang_pref) + " –û–¥–Ω–∞ –∫–æ—Ä–æ—Ç–∫–∞—è –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–∞—è —Ä–µ–ø–ª–∏–∫–∞ (8‚Äì14 —Å–ª–æ–≤)."},
        {"role": "user", "content": scrub_for_policy(
            topics_blob) if topics_blob else "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–¥–µ–ª–∏: –º–æ–¥–∞, –ø–æ–∫–∞–∑—ã, –∫–∞–º–ø–∞–Ω–∏–∏, —Ä–æ–∑–Ω–∏—Ü–∞."},
    ], temperature=0.5).strip()
    if not raw:
        return "–°—Ç–∏–ª—å ‚Äî —ç—Ç–æ –≤—ã–±–æ—Ä –Ω–∞ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å, –∞ –Ω–µ –∫–∞–ª–µ–Ω–¥–∞—Ä—å."
    raw = raw.strip(" \n\"'‚Äú‚Äù‚Äò‚Äô")
    raw = re.sub(r"\s+", " ", raw)
    raw = re.sub(r":\s*$", "", raw)
    return _clip(raw, 120)


# ---------- –û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –¥–ª—è Telegram ----------
def summarize_line(title: str, text: str, lang_pref: str = "ru") -> str:
    blob = (text or "").strip()
    if OPENROUTER_API_KEY and len(blob) > 60:
        raw = llm_call([
            {"role": "system", "content":
             safer_tone_prompt(lang_pref) +
             " –í–µ—Ä–Ω–∏ –û–î–ù–£ –∂–∏–≤—É—é —Å—Ç—Ä–æ–∫—É-–Ω–æ–≤–æ—Å—Ç—å (‚âà12‚Äì20 —Å–ª–æ–≤), –∫–∞–∫ –≤ –º–æ–¥–Ω–æ–º –∂—É—Ä–Ω–∞–ª–µ. "
             "–ë–µ–∑ –±—É–ª–ª–µ—Ç–æ–≤, –∫–∞–≤—ã—á–µ–∫, —ç–º–æ–¥–∑–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. "
             "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞–≥–æ–ª–æ–≤–æ–∫. –ù–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è."},
            {"role": "user",
                "content": f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {clean_title(title)}\n–¢–µ–∫—Å—Ç: {scrub_for_policy(blob)[:1200]}"},
        ], temperature=0.5).strip()

        if raw:
            s = raw.strip(" \n\"'‚Äú‚Äù‚Äò‚Äô‚Ä¢-‚Äî‚Äì")
            s = re.sub(r"\s+", " ", s)
            tclean = clean_title(title)
            if tclean and tclean.lower() in s.lower():
                s = re.sub(re.escape(tclean), "", s,
                           flags=re.IGNORECASE).strip(" .,:;‚Äî‚Äì-")
            s = s.replace(":", " ").replace(" ‚Äî ", " ").replace("‚Äì", " ")
            s = vary_verbs(s)
            words = s.split()
            if len(words) > 20:
                s = " ".join(words[:20]) + "‚Ä¶"
            return s

    sents = [x.strip()
             for x in re.split(r"(?<=[.!?])\s+", blob) if len(x.split()) > 6]
    s = sents[0] if sents else clean_title(title)
    s = re.sub(r"[:‚Äî‚Äì-]\s*$", "", s)
    s = vary_verbs(s)
    words = s.split()
    if len(words) > 20:
        s = " ".join(words[:20]) + "‚Ä¶"
    return s


# ---------- –ü–∞—Ä–∞ –¥–ª—è Telegraph ----------
def summarize_pair(title: str, text: str, lang_pref: str = "ru") -> tuple[str, str]:
    hook, desc = "", ""
    blob = scrub_for_policy(text or "")
    tclean = clean_title(title)

    if OPENROUTER_API_KEY and len(blob) > 120:
        raw = llm_call([
            {"role": "system", "content":
             safer_tone_prompt(lang_pref) +
             " –†–û–í–ù–û –¥–≤–µ —Å—Ç—Ä–æ–∫–∏: 1) —Ö—É–∫ (–æ–±—Ä–∞–∑–Ω–æ, –Ω–æ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ); 2) –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–∫—Ç/–¥–µ—Ç–∞–ª—å. "
             "–ë–µ–∑ –∏–º—ë–Ω –ø–µ—Ä—Å–æ–Ω (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –±—Ä–µ–Ω–¥), –±–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏–π –∏ —ç–º–æ–¥–∑–∏. "
             "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∏ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ."},
            {"role": "user",
                "content": f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {tclean}\n–¢–µ–∫—Å—Ç: {blob[:6000]}"},
        ], temperature=0.5).strip()

        if raw:
            parts = [p.strip(" ‚Ä¢-‚Äì‚Äî\"'‚Äú‚Äù‚Äò‚Äô")
                     for p in re.split(r"\n+", raw) if p.strip()]
            hook = parts[0] if parts else ""
            desc = parts[1] if len(parts) > 1 else ""

    def _sanitize(s: str) -> str:
        s = re.sub(r"\s+", " ", (s or "").strip())
        if tclean and tclean.lower() in s.lower():
            s = re.sub(re.escape(tclean), "", s,
                       flags=re.IGNORECASE).strip(" .,:;‚Äî‚Äì-")
        return s.replace(":", " ")

    sents = [s.strip()
             for s in re.split(r"(?<=[.!?])\s+", blob) if len(s.split()) > 6]
    hook = _sanitize(hook) or (sents[0] if sents else "")
    desc = _sanitize(desc)
    if not desc or desc.lower() == hook.lower():
        desc = next((s for s in sents[1:] if s and s.lower() not in (
            hook.lower(), tclean.lower())), "")
    if desc and (desc.lower() == hook.lower() or fuzz.partial_ratio(hook, desc) > 90):
        desc = _clip(desc + " –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ ‚Äî –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–µ.", 160)

    hook = _postprocess_line(title, vary_verbs(hook), max_len=120)
    desc = _postprocess_line(title, vary_verbs(desc), max_len=160)
    return hook, desc


# ---------- –ß—Ç–µ–Ω–∏–µ –∏–∑ TG-–∫–∞–Ω–∞–ª–æ–≤ ----------
def extract_button_url_and_text(msg):
    try:
        if msg and msg.reply_markup and msg.reply_markup.rows:
            for row in msg.reply_markup.rows:
                for btn in row.buttons:
                    url = getattr(btn, "url", None)
                    text = getattr(btn, "text", None)
                    if url and text:
                        return text.strip(), url
    except Exception:
        pass
    return None, None


async def fetch_tg_items(since: datetime):
    if not (TG_API_ID and TG_API_HASH and TG_STRING_SESSION):
        logger.warning("Telethon creds missing; skip TG.")
        return []
    items = []
    async with TelegramClient(StringSession(TG_STRING_SESSION), TG_API_ID, TG_API_HASH) as client:
        for uname in TG_CHANNELS:
            try:
                entity = await client.get_entity(uname)
                raw_msgs = []
                async for m in client.iter_messages(entity, limit=600):
                    if not m:
                        continue
                    raw_msgs.append(m)

                # –°–∫–ª–µ–π–∫–∞ –∞–ª—å–±–æ–º–æ–≤
                groups, singles = {}, []
                for m in raw_msgs:
                    if m.grouped_id:
                        groups.setdefault(m.grouped_id, []).append(m)
                    else:
                        singles.append(m)
                merged = []
                for gid, msgs in groups.items():
                    msgs.sort(key=lambda x: len(
                        (x.message or "").strip()), reverse=True)
                    merged.append(msgs[0])
                merged.extend(singles)

                count_for_chan = 0
                for msg in merged:
                    d = (msg.date.replace(tzinfo=None)
                         if msg.date else datetime.now()).replace(microsecond=0)
                    if d < since:
                        continue

                    text_full = (msg.message or "").strip()
                    if not is_soft_relevant(text_full):
                        continue

                    btn_text, btn_url = extract_button_url_and_text(msg)
                    title_candidate = (text_full.split("\n", 1)[
                                       0] if text_full else "").strip()
                    if not title_candidate and btn_text:
                        title_candidate = btn_text.strip()
                    if not title_candidate and text_full:
                        title_candidate = text_full[:80].strip()
                    if not title_candidate and not btn_url:
                        continue

                    title = clean_title(title_candidate or "")
                    if not title:
                        title = clean_title(
                            (text_full[:80] if text_full else btn_text or "") or "")
                    if not title:
                        continue

                    permalink = btn_url or f"https://t.me/{uname}/{msg.id}"

                    items.append({
                        "title": title,
                        "summary": (text_full or title)[:400],
                        "link": permalink,
                        "published": d,
                        "source": "tg",
                        "channel": uname,
                        "full_text": text_full or title,
                    })
                    count_for_chan += 1

                logger.info(f"TG @{uname}: —Å–æ–±—Ä–∞–Ω–æ {count_for_chan} –ø–æ—Å—Ç–æ–≤")
            except ChannelPrivateError:
                logger.warning(
                    f"TG fetch @{uname} failed: private channel (–Ω—É–∂–Ω–∞ –ø–æ–¥–ø–∏—Å–∫–∞ —ç—Ç–∏–º –∞–∫–∫–∞—É–Ω—Ç–æ–º).")
            except ChatAdminRequiredError:
                logger.warning(
                    f"TG fetch @{uname} failed: –Ω–µ—Ç –ø—Ä–∞–≤ —á–∏—Ç–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é.")
            except Exception as e:
                logger.warning(f"TG fetch @{uname} failed: {e}")
    return items


# ---------- –°–±–æ—Ä –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ----------
async def gather_candidates():
    since = (datetime.now(timezone.utc) -
             timedelta(days=LOOKBACK_DAYS)).replace(tzinfo=None)
    tg_items = await fetch_tg_items(since)
    items = tg_items
    items.sort(key=lambda x: x["published"], reverse=True)
    return dedupe(items)


def pick_emoji(title: str, text: str) -> str:
    blob = f"{title}\n{text}".lower()
    if any(k in blob for k in ["runway", "–ø–æ–∫–∞–∑", "fashion week", "–ø–æ–¥–∏—É–º", "–Ω–µ–¥–µ–ª—è –º–æ–¥—ã"]):
        return "üëó"
    if any(k in blob for k in ["–∫–æ–ª–ª–µ–∫—Ü", "collection", "–∫–∞–ø—Å—É–ª", "–∫–∞–ø—Å—É–ª–∞", "lookbook", "–ª—É–∫–±—É–∫", "drop", "–¥—Ä–æ–ø"]):
        return "üßµ"
    if any(k in blob for k in ["—Ä–µ—Ç–µ–π–ª", "–º–∞–≥–∞–∑–∏–Ω", "retail", "–º–∞—Ä–∫–µ—Ç", "–ø—Ä–æ–¥–∞–∂"]):
        return "üõçÔ∏è"
    if any(k in blob for k in ["–≤–∏–¥–µ–æ", "video", "–∫–∞–º–ø–∞–Ω–∏—è", "campaign", "–∫—Ä–µ–∞—Ç–∏–≤", "—Å—ä—ë–º–∫", "—Å—ä–µ–º–∫"]):
        return "üé¨"
    if any(k in blob for k in ["–¥–∂–∏–Ω—Å", "–±—Ä—é–∫–∏", "–ø–∞–ª—å—Ç–æ", "–ø–∏–¥–∂–∞–∫", "—Å—É–º–∫–∞", "–∞–∫—Å–µ—Å—Å—É–∞—Ä", "–æ–±—É–≤—å", "–∫—Ä–æ—Å—Å–æ–≤", "–±–æ—Ç–∏–Ω–∫", "heels"]):
        return "üß•"
    if any(k in blob for k in ["–±–µ–ª—å–µ", "–±–µ–ª—å—ë", "lingerie", "bra", "bralette", "–∫–æ—Ä—Å–µ—Ç", "bodysuit", "swimwear"]):
        return "ü©±"
    return "‚ú®"


async def build_digest(max_items=None):
    hard_cap = 6
    max_items = min(ITEMS_MAX or hard_cap, hard_cap) if max_items is None else min(
        max_items, hard_cap)
    cand = await gather_candidates()
    out = []
    for it in cand:
        if len(out) >= max_items:
            break
        text_for_sum = it.get("full_text") or it.get("summary", "")
        hook, desc = summarize_pair(
            it["title"], text_for_sum, LANG_PREF)   # –¥–ª—è Telegraph
        line = summarize_line(it["title"], text_for_sum,
                              LANG_PREF)         # –¥–ª—è Telegram
        out.append({**it, "hook": hook, "desc": desc, "line": line})
    return out


# ---------- –£–º–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è TELEGRAM (Markdown) ----------
CAPITAL_PHRASE_RE = re.compile(
    r"\b([A-Z–ê-–Ø–Å][\w\-]{2,}(?:\s+[A-Z–ê-–Ø–Å][\w\-]{2,}){0,2})\b")


def _mk_link(text: str, url: str) -> str:
    """
    –í–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π ¬´—Å—Ç—Ä–æ–∫–∏-–Ω–æ–≤–æ—Å—Ç–∏¬ª –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –∫–∞–ø–∏—Ç–∞–ª—å–Ω—É—é —Ñ—Ä–∞–∑—É
    (–∏–ª–∏ –ø–µ—Ä–≤—ã–µ 3‚Äì5 —Å–ª–æ–≤) –≤ [–∫–ª–∏–∫–∞–±–µ–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É](url) –¥–ª—è Telegram Markdown.
    """
    s = (text or "").strip()
    if not s or not url:
        return s

    # –∏–∑–±–µ–≥–∞–µ–º Already-linked
    if "](" in s:
        return s

    # –≤—ã–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–∑—É
    m = CAPITAL_PHRASE_RE.search(s)
    if m:
        a, b = m.span()
        before = s[:a]
        mid = m.group(0)
        after = s[b:]
    else:
        words = s.split()
        n = min(5, max(3, len(words)//4 or 3))
        mid = " ".join(words[:n])
        before = ""
        after = " ".join(words[n:])
        if after:
            after = " " + after  # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–±–µ–ª

    # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –≤ Markdown
    mid = mid.replace("[", "(").replace("]", ")")
    linked = f"{before}[{mid}]({url}){after}"
    return linked


# ---------- Telegraph (–±–µ–∑ ¬´—É–º–Ω—ã—Ö —Å—Å—ã–ª–æ–∫¬ª, –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç) ----------
def tgraph_create_account(short_name: str, author_name: str = "", author_url: str = "") -> dict:
    try:
        r = requests.post(f"{TELEGRAPH_API}/createAccount", data={
            "short_name": short_name[:32] or "digest",
            "author_name": author_name or "",
            "author_url": author_url or "",
        }, timeout=20)
        return r.json()
    except Exception as e:
        logger.warning(f"Telegraph createAccount failed: {e}")
        return {"ok": False, "error": str(e)}


def tgraph_create_page(access_token: str, title: str, nodes: list, author_name: str = "", author_url: str = "") -> dict:
    try:
        payload = {
            "access_token": access_token,
            "title": (title or "")[:256],
            "author_name": author_name or "",
            "author_url": author_url or "",
            "content": json.dumps(nodes, ensure_ascii=False),
            "return_content": False,
        }
        r = requests.post(f"{TELEGRAPH_API}/createPage",
                          data=payload, timeout=30)
        return r.json()
    except Exception as e:
        logger.warning(f"Telegraph createPage failed: {e}")
        return {"ok": False, "error": str(e)}


def create_digest_page(title: str, items: list) -> str | None:
    try:
        token = os.environ.get("TELEGRAPH_ACCESS_TOKEN")
        if not token:
            acc = tgraph_create_account(short_name=(TELEGRAPH_AUTHOR_NAME or "digest")[:32],
                                        author_name=TELEGRAPH_AUTHOR_NAME or "",
                                        author_url=TELEGRAPH_AUTHOR_URL or "")
            if acc.get("ok") and acc.get("result"):
                token = acc["result"].get("access_token")
                if token:
                    os.environ["TELEGRAPH_ACCESS_TOKEN"] = token
        if not token:
            return None

        nodes = []
        def add(n): nodes.append(n)

        add({"tag": "h2", "children": [title]})
        # spacer, —á—Ç–æ–±—ã –ø—Ä–µ–≤—å—é –Ω–µ —Å–∫–ª–µ–∏–≤–∞–ª–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º –∞–±–∑–∞—Ü–µ–º
        add({"tag": "p", "children": [" "]})
        add({"tag": "p", "children": [
            "–ù–µ–¥–µ–ª—è –≤ –º–æ–¥–µ: –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –ø–æ–∫–∞–∑—ã, –∫–∞–º–ø–∞–Ω–∏–∏ –∏ —Ä–æ–∑–Ω–∏—Ü–∞. –ù–∏–∂–µ ‚Äî –≤—ã–∂–∏–º–∫–∏ –∏ –º–∞–∫—Å–∏–º—É–º –¥–µ—Ç–∞–ª–µ–π."
        ]})
        add({"tag": "h3", "children": ["üß≠ –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ"]})
        toc = []
        for i, it in enumerate(items, 1):
            toc.append({"tag": "li", "children": [
                {"tag": "a", "attrs": {"href": it["link"]}, "children": [
                    f"{i}. {it['title']}"]}
            ]})
        add({"tag": "ul", "children": toc})

        used = 0
        SAFE = 61000

        def ok(s: str) -> bool:
            nonlocal used
            inc = len(s)
            if used + inc > SAFE:
                return False
            used += inc
            return True

        for it in items:
            emoji = pick_emoji(it.get("title", ""), it.get("hook", ""))
            add({"tag": "h3", "children": [
                emoji + " ",
                {"tag": "a", "attrs": {
                    "href": it["link"]}, "children": [it["title"]]}
            ]})
            if it.get("hook"):
                add({"tag": "p", "children": [it["hook"]]})
                ok(it["hook"])
            if it.get("desc"):
                add({"tag": "p", "children": [it["desc"]]})
                ok(it["desc"])

            full_text = (it.get("full_text") or "").strip()
            if full_text:
                for p in [p.strip() for p in full_text.split("\n") if p.strip()]:
                    if not ok(p):
                        break
                    add({"tag": "p", "children": [p]})

        resp = tgraph_create_page(
            access_token=token, title=title, nodes=nodes,
            author_name=TELEGRAPH_AUTHOR_NAME, author_url=TELEGRAPH_AUTHOR_URL
        )
        if resp.get("ok") and resp.get("result"):
            return resp["result"].get("url")
    except Exception as e:
        logger.warning(f"Digest page failed: {e}")
    return None


# ---------- Telegram –≤—ã–≤–æ–¥ ----------
IRONIC_FALLBACKS = [
    "–ü–æ—Ö–æ–∂–µ, –∏–Ω–¥—É—Å—Ç—Ä–∏—è –≤–∑—è–ª–∞ –≤—ã—Ö–æ–¥–Ω–æ–π. –ú–æ–¥–∞ –º–æ–ª—á–∏—Ç.",
    "–†–µ–¥–∫–∏–π —Å–ª—É—á–∞–π: –Ω–µ–¥–µ–ª—è —Ç–∏—à–µ, —á–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–æ—á—å—é.",
    "–ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç ‚Äî –º–æ–∂–Ω–æ —Å–ø–æ–∫–æ–π–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∫–∞–ø—Å—É–ª—É.",
    "–õ–µ–Ω—Ç—ã –ø—É—Å—Ç—ã, –∑–∞—Ç–æ –≤–∏—Ç—Ä–∏–Ω—ã ‚Äî –Ω–µ—Ç. –ü—Ä–æ–≥—É–ª—è–µ–º—Å—è?"
]


async def send_typing(bot, chat_id):
    try:
        await bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
    except Exception:
        pass


def render_message(items, digest_url: str | None = None):
    if not items:
        return random.choice(IRONIC_FALLBACKS)

    # –¢–û–õ–¨–ö–û –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –ø—É–Ω–∫—Ç—ã. –ù–∏–∫–∞–∫–∏—Ö –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π/¬´–ª–∏–¥–æ–≤¬ª.
    title = "–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç"
    lines = [f"*{title}*"]

    # –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —É–º–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
    seen_lines = []
    for it in items:
        emoji = pick_emoji(it.get("title", ""), it.get(
            "line", "") or it.get("hook", ""))
        raw_line = (it.get("line") or "").replace(":", " ").strip()
        # –∞–Ω—Ç–∏-–¥—É–±–ª–∏
        if any(fuzz.partial_ratio(raw_line, prev) > 92 for prev in seen_lines):
            continue
        seen_lines.append(raw_line)
        linked_line = _mk_link(raw_line, it.get("link", ""))
        lines.append(f"{emoji} {linked_line}")

    if digest_url:
        lines.append(f"[–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é]({digest_url})")

    return "\n\n".join(lines).strip()


async def post_digest(bot):
    await send_typing(bot, CHANNEL_ID)
    try:
        items = await build_digest()
        digest_url = create_digest_page(
            "–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç", items) if items else None
        text = render_message(items, digest_url)
        await bot.send_message(
            chat_id=CHANNEL_ID,
            text=text,
            parse_mode=ParseMode.MARKDOWN,
            disable_web_page_preview=False,
        )
        logger.info(
            f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {len(items)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤; telegraph={bool(digest_url)}")
    except Exception as e:
        logger.exception(e)
        await bot.send_message(chat_id=CHANNEL_ID, text=f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")


# ---------- –ö–æ–º–∞–Ω–¥—ã ----------
async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! /preview ‚Äî —á–µ—Ä–Ω–æ–≤–∏–∫ –º–æ–¥–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞.")


async def cmd_preview(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await send_typing(context.bot, update.effective_chat.id)
    try:
        items = await build_digest()
        digest_url = create_digest_page(
            "–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç (—á–µ—Ä–Ω–æ–≤–∏–∫)", items) if items else None
        text = render_message(items, digest_url)
        await update.message.reply_text(text, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=False)
    except Exception as e:
        logger.exception(e)
        await update.message.reply_text(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–≤—å—é: {e}")


# ---------- –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ—Ç–∫–ª—é—á—ë–Ω (CRON —Å–Ω–∞—Ä—É–∂–∏) ----------
# –ó–∞–ø—É—Å–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–Ω–µ—à–Ω–∏–º CRON: POST_ONCE=1 python3 main.py


# ---------- –†–µ–∂–∏–º—ã –∑–∞–ø—É—Å–∫–∞ ----------
async def preview_console():
    items = await build_digest()
    digest_url = create_digest_page(
        "–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç (–∫–æ–Ω—Å–æ–ª—å)", items) if items else None
    print(render_message(items, digest_url))


async def post_once():
    request = HTTPXRequest(
        connect_timeout=30.0, read_timeout=30.0, write_timeout=30.0, pool_timeout=30.0)
    app = Application.builder().token(BOT_TOKEN).request(request).build()
    await app.initialize()
    try:
        await post_digest(app.bot)
    finally:
        try:
            await app.shutdown()
        except Exception:
            pass


def main():
    if os.environ.get("PREVIEW_CONSOLE") == "1" or (len(sys.argv) > 1 and sys.argv[1].lower() == "preview"):
        asyncio.run(preview_console())
        return
    if os.environ.get("POST_ONCE") == "1" or (len(sys.argv) > 1 and sys.argv[1].lower() == "post"):
        if not BOT_TOKEN or not CHANNEL_ID:
            raise RuntimeError(
                "–ù—É–∂–Ω—ã TELEGRAM_BOT_TOKEN –∏ TELEGRAM_CHANNEL_ID")
        asyncio.run(post_once())
        return

    # –†–µ–∂–∏–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ polling –æ—Ç–∫–ª—é—á—ë–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CRON —Å POST_ONCE=1.
    print("CRON-—Ä–µ–∂–∏–º: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ POST_ONCE=1 –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, PREVIEW_CONSOLE=1 –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞.")


if __name__ == "__main__":
    main()
